# LLM-Use Model Configuration
models:
  # OpenAI Models
  gpt-3.5-turbo:
    name: "GPT-3.5 Turbo"
    provider: "openai"
    cost_per_1k_input: 0.0005
    cost_per_1k_output: 0.0015
    quality: 7
    speed: "fast"
    context_window: 16385
    supports_streaming: true
    best_for: ["general", "simple_tasks", "chat"]
    
  gpt-4-turbo-preview:
    name: "GPT-4 Turbo"
    provider: "openai"
    cost_per_1k_input: 0.01
    cost_per_1k_output: 0.03
    quality: 10
    speed: "medium"
    context_window: 128000
    supports_streaming: true
    best_for: ["complex", "reasoning", "coding", "analysis"]
  
  # Anthropic Models
  claude-3-haiku-20240307:
    name: "Claude 3 Haiku"
    provider: "anthropic"
    cost_per_1k_input: 0.00025
    cost_per_1k_output: 0.00125
    quality: 7
    speed: "fast"
    context_window: 200000
    supports_streaming: true
    best_for: ["general", "chat", "summarization"]
  
  claude-3-opus-20240229:
    name: "Claude 3 Opus"
    provider: "anthropic"
    cost_per_1k_input: 0.015
    cost_per_1k_output: 0.075
    quality: 10
    speed: "slow"
    context_window: 200000
    supports_streaming: true
    best_for: ["analysis", "writing", "coding", "research"]
  
  # Google Models  
  gemini-pro:
    name: "Gemini Pro"
    provider: "google"
    cost_per_1k_input: 0.00025
    cost_per_1k_output: 0.0005
    quality: 8
    speed: "fast"
    context_window: 30720
    supports_streaming: true
    best_for: ["general", "analysis", "multimodal"]
  
  # Groq Models
  llama-3.1-70b-versatile:
    name: "Llama 3.1 70B (Groq)"
    provider: "groq"
    cost_per_1k_input: 0.00059
    cost_per_1k_output: 0.00079
    quality: 9
    speed: "ultra_fast"
    context_window: 8192
    supports_streaming: true
    best_for: ["fast", "general", "reasoning"]
  
  mixtral-8x7b-32768:
    name: "Mixtral 8x7B (Groq)"
    provider: "groq"
    cost_per_1k_input: 0.00024
    cost_per_1k_output: 0.00024
    quality: 8
    speed: "ultra_fast"
    context_window: 32768
    supports_streaming: true
    best_for: ["fast", "general", "coding"]
  
  # Local Models (Ollama) - QUESTI RIMANGONO UGUALI
  gemma3:27b:
    name: "Gemma3 27B"
    provider: "ollama"
    cost_per_1k_input: 0.0
    cost_per_1k_output: 0.0
    quality: 10
    speed: "slow"
    context_window: 8192
    supports_streaming: true
    best_for: ["complex_reasoning", "analysis", "coding", "local", "private"]
  
  gemma3:12b:
    name: "Gemma3 12B"
    provider: "ollama"
    cost_per_1k_input: 0.0
    cost_per_1k_output: 0.0
    quality: 8  # Buon modello medio-grande
    speed: "medium"  # 12B velocità media
    context_window: 8192
    supports_streaming: true
    best_for:
      - "general"
      - "coding"
      - "analysis"
      - "local"
      - "private"
  
  llama3.1:8b:
    name: "Llama 3.1 8B"
    provider: "ollama"
    cost_per_1k_input: 0.0
    cost_per_1k_output: 0.0
    quality: 9  # Llama 3.1 è eccellente nonostante 8B
    speed: "fast"  # 8B è veloce
    context_window: 128000  # Llama 3.1 ha context enorme!
    supports_streaming: true
    best_for:
      - "general"
      - "coding"
      - "chat"
      - "reasoning"
      - "local"
      - "private"
  
  deepseek-r1:7b:
    name: "DeepSeek R1 7B"
    provider: "ollama"
    cost_per_1k_input: 0.0
    cost_per_1k_output: 0.0
    quality: 9  # Ottimizzato per reasoning
    speed: "fast"  # 7B è veloce
    context_window: 32768
    supports_streaming: true
    best_for:
      - "reasoning"
      - "math"
      - "coding"
      - "logic"
      - "local"
      - "private"
  
  mistral:latest:
    name: "Mistral 7B"
    provider: "ollama"
    cost_per_1k_input: 0.0
    cost_per_1k_output: 0.0
    quality: 8  # Buon modello generale
    speed: "fast"
    context_window: 32768
    supports_streaming: true
    best_for:
      - "general"
      - "chat"
      - "coding"
      - "local"
      - "private"
  
  gemma3:1b:
    name: "Gemma3 1B"
    provider: "ollama"
    cost_per_1k_input: 0.0
    cost_per_1k_output: 0.0
    quality: 3  # Modello piccolo
    speed: "ultra_fast"
    context_window: 8192
    supports_streaming: true
    best_for:
      - "simple_tasks"
      - "quick_responses"
      - "chat"
      - "local"
      - "private"
  
  gemma3:270m:
    name: "Gemma3 270M"
    provider: "ollama"
    cost_per_1k_input: 0.0
    cost_per_1k_output: 0.0
    quality: 1  # Modello molto piccolo
    speed: "ultra_fast"
    context_window: 8192
    supports_streaming: true
    best_for:
      - "greetings"
      - "simple_qa"
      - "ultra_fast"
      - "local"
      - "private"

# Routing Rules (optional overrides)
routing_rules:
  complexity_thresholds:
    simple: 3
    moderate: 6
    complex: 10
  overrides: []

# Provider Settings
providers:
  openai:
    api_key_env: "OPENAI_API_KEY"
    timeout: 30
    base_url: "https://api.openai.com/v1"
    
  anthropic:
    api_key_env: "ANTHROPIC_API_KEY"
    timeout: 30
    base_url: "https://api.anthropic.com"
    
  google:
    api_key_env: "GOOGLE_API_KEY"
    timeout: 30